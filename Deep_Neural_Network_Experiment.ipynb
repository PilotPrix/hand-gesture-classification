{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_bwgzHgo3JS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim \n",
        "import numpy as np  # to load dataset\n",
        "import math\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHabcSsao2TJ",
        "outputId": "fdcc01dc-cc63-42d6-fbc5-99604db59a64"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device in use: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the dataset\n",
        "# 0 corresponds to rotating\n",
        "# 1 corresponds to grasping\n",
        "\n",
        "# num rows for one grasp\n",
        "block_size = 2 * 24\n",
        "\n",
        "#linux command\n",
        "# %cd /content/drive/MyDrive/PyTorch/\n",
        "\n",
        "def normalize_vector(nd_arr: np.ndarray) -> np.ndarray:\n",
        "    vector_length = sum([i ** 2 for i in nd_arr]) ** 0.5\n",
        "    normalized_arr = [i / vector_length for i in nd_arr]\n",
        "    return normalized_arr\n",
        "\n",
        "# Rotation\n",
        "unedited_rotating_dataset = np.loadtxt('rotating.csv', delimiter=\",\")\n",
        "# Split into multiple movements over time (\"block_size\" seconds)\n",
        "edited_rotating_dataset = unedited_rotating_dataset[:,1:21] #corresponding to cols 2 - 21 inclusive, excluding the last row of zeros\n",
        "# Convert positions into velocities\n",
        "edited_rotating_dataset = np.diff(edited_rotating_dataset, axis=0)\n",
        "# Normalize each row\n",
        "edited_rotating_dataset = np.array([normalize_vector(row) for row in edited_rotating_dataset])\n",
        "\n",
        "# Grasping\n",
        "unedited_grasping_dataset = np.loadtxt('grasping.csv', delimiter=\",\")\n",
        "edited_grasping_dataset = unedited_grasping_dataset[:,1:21]\n",
        "edited_grasping_dataset = np.diff(edited_grasping_dataset, axis=0)\n",
        "# Normalize\n",
        "edited_rotating_dataset = np.array([normalize_vector(row) for row in edited_grasping_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPQUdDtEyXST"
      },
      "outputs": [],
      "source": [
        "# Rotating\n",
        "rows = edited_rotating_dataset.shape[0]\n",
        "X = np.zeros((rows - block_size + 1, block_size * 20))  # (Block size * 24) seconds times 20 rows (from flattening)\n",
        "\n",
        "# For each block\n",
        "for i in range(0, rows - block_size + 1):\n",
        "  # Get entire row, 1st block_size * 24 rows, 2nd of them and so on..\n",
        "  X[i] = np.ndarray.flatten(edited_rotating_dataset[i : i + block_size, :])\n",
        "\n",
        "# X now contains mutliple blocks\n",
        "y = np.zeros((rows - block_size + 1))\n",
        "\n",
        "\n",
        "# grasping\n",
        "rows = edited_grasping_dataset.shape[0]\n",
        "X2 = np.zeros((rows - block_size + 1, block_size * 20))\n",
        "# For each block\n",
        "for i in range(0, rows - block_size + 1):\n",
        "  # Get entire row, 1st, block_size * 24 rows, second of them and so on..\n",
        "  X2[i] = np.ndarray.flatten(edited_grasping_dataset[i : i + block_size, :])\n",
        "# X now contains mutliple blocks\n",
        "y2 = np.ones((rows - block_size + 1))\n",
        "combined_X = np.concatenate((X, X2), axis=0)\n",
        "combined_y = np.concatenate((y, y2), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0SU43jpH6Vt",
        "outputId": "1fa268dc-e648-4e85-f7c9-86ca09d1e9b9"
      },
      "outputs": [],
      "source": [
        "# covnert into tensors since numpy uses 64 bit floating point\n",
        "X = torch.tensor(combined_X, dtype=torch.float32)\n",
        "y = torch.tensor(combined_y, dtype=torch.float32)\n",
        "y = torch.reshape(y,(-1,1)) # a single dimension uses -1 to infer the length. new shape\n",
        "\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVsm3hfno_-i"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(block_size * 20, 64), # input\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9v3oCcTp-0q",
        "outputId": "97907e39-6ff0-4b24-c554-c52e4231cfad"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "X.to(device)\n",
        "y.to(device)\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8QKRiXTGL4wC"
      },
      "source": [
        "## Preparation for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpdsuy9KLsvl"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.BCELoss() # used for binary classification problems\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQA9YX0nOpD0",
        "outputId": "d388f7a2-9ae4-42b4-a455-efb27783e934"
      },
      "outputs": [],
      "source": [
        "n_epochs = 15\n",
        "batch_size = 4\n",
        "\n",
        "# CPU\n",
        "if device == \"cpu\":\n",
        "  for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X) - batch_size, batch_size):\n",
        "      Xbatch = X[i:i+batch_size] # specifying the rows\n",
        "      y_pred = model(Xbatch) # scalar\n",
        "      ybatch = y[i:i+batch_size]\n",
        "      loss = loss_fn(y_pred, ybatch)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f\"epoch: {epoch + 1}/{n_epochs}\", end=\"\\r\")\n",
        "  print(f'CPU: Finished epoch {epoch}, latest loss {loss}')\n",
        "\n",
        "# GPU\n",
        "elif device == \"cuda\":\n",
        "  for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X) - batch_size, batch_size):\n",
        "      Xbatch = X[i:i+batch_size] # specifying the rows\n",
        "      y_pred = model(Xbatch.cuda()) # scalar\n",
        "      ybatch = y[i:i+batch_size].cuda()\n",
        "      loss = loss_fn(y_pred, ybatch)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    print(f\"epoch: {epoch + 1}/{n_epochs}\", end=\"\\r\")\n",
        "  print(f'CUDA: Finished epoch {epoch}, latest loss {loss}')\n",
        "else:\n",
        "  print(\"Not using CPU or CUDA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEcBZ3U9aYrA"
      },
      "outputs": [],
      "source": [
        "# evaluate the accuracy of the model\n",
        "\n",
        "with torch.no_grad(): # no_grad to avoid differentiating\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    y_pred = model(X.to(device))\n",
        "\n",
        "# rounds prediction to nearest integer, check if equal to y, \n",
        "accuracy = (y_pred.round() == y).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
